{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Cost Analysis EDA\n",
    "\n",
    "**Objective:** Calculate the *actual* cost of classifying player-mention comments via Claude Haiku 4.5 Batch API.\n",
    "\n",
    "**Key Question:** At $200 budget, how many comments can we classify?\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. Sample 1,000 comments from `r_nba_cleaned.jsonl`\n",
    "2. Build actual batch request payloads (system prompt + user message)\n",
    "3. Count tokens via Anthropic's `count_tokens()` API\n",
    "4. Estimate output tokens based on response schema\n",
    "5. Calculate cost at Batch API pricing (50% discount)\n",
    "\n",
    "**Batch API Pricing (Haiku 4.5):**\n",
    "\n",
    "| Component | Standard | Batch (50% off) |\n",
    "|-----------|----------|------------------|\n",
    "| Input | $1.00/MTok | **$0.50/MTok** |\n",
    "| Output | $5.00/MTok | **$2.50/MTok** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments file exists: True\n",
      "Players config exists: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import anthropic\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"data\")\n",
    "COMMENTS_PATH = DATA_DIR / \"filtered\" / \"r_nba_cleaned.jsonl\"\n",
    "PLAYERS_CONFIG = Path(\"config/players.yaml\")\n",
    "\n",
    "# Anthropic client\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "print(f\"Comments file exists: {COMMENTS_PATH.exists()}\")\n",
    "print(f\"Players config exists: {PLAYERS_CONFIG.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-section",
   "metadata": {},
   "source": [
    "## 1. Load Player Config\n",
    "\n",
    "Fresh implementation based on `config/players.yaml` with cleaned-up matching logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "load-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Players tracked: 92\n",
      "Short aliases (need word boundary): 41\n",
      "Short aliases: ['ad', 'ai', 'ant', 'bam', 'bi', 'book', 'brown', 'cp', 'curry', 'dame', 'davis', 'dg', 'edwards', 'fox', 'george', 'gordon', 'green', 'hart', 'holiday', 'howard', 'ja', 'james', 'jordan', 'jp', 'kat', 'kd', 'leonard', 'mitchell', 'murray', 'og', 'paul', 'pg', 'rose', 'sga', 'smart', 'the greek', 'tt', 'turner', 'wall', 'zo', 'zu']\n"
     ]
    }
   ],
   "source": [
    "with open(PLAYERS_CONFIG) as f:\n",
    "    player_config = yaml.safe_load(f)\n",
    "\n",
    "PLAYER_ALIASES = player_config[\"players\"]\n",
    "SHORT_ALIASES = set(player_config.get(\"short_aliases\", []))\n",
    "\n",
    "print(f\"Players tracked: {len(PLAYER_ALIASES)}\")\n",
    "print(f\"Short aliases (need word boundary): {len(SHORT_ALIASES)}\")\n",
    "print(f\"Short aliases: {sorted(SHORT_ALIASES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "alias-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total aliases across all players: 346\n",
      "Average aliases per player: 3.8\n",
      "\n",
      "LeBron James aliases (21):\n",
      "  - lebron\n",
      "  - bron\n",
      "  - lbj\n",
      "  - james\n",
      "  - the king\n",
      "  - chosen one\n",
      "  - lemickey\n",
      "  - lebrick\n",
      "  - lechoke\n",
      "  - leflop\n",
      "  ... and 11 more\n"
     ]
    }
   ],
   "source": [
    "# Audit: Count total aliases and redundancy\n",
    "total_aliases = sum(len(aliases) for aliases in PLAYER_ALIASES.values())\n",
    "print(f\"Total aliases across all players: {total_aliases}\")\n",
    "print(f\"Average aliases per player: {total_aliases / len(PLAYER_ALIASES):.1f}\")\n",
    "\n",
    "# Show a sample player's aliases\n",
    "sample_player = \"LeBron James\"\n",
    "print(f\"\\n{sample_player} aliases ({len(PLAYER_ALIASES[sample_player])}):\")\n",
    "for alias in PLAYER_ALIASES[sample_player][:10]:\n",
    "    print(f\"  - {alias}\")\n",
    "if len(PLAYER_ALIASES[sample_player]) > 10:\n",
    "    print(f\"  ... and {len(PLAYER_ALIASES[sample_player]) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matcher-section",
   "metadata": {},
   "source": [
    "## 2. Player Matcher (Fresh Implementation)\n",
    "\n",
    "Rules:\n",
    "1. Aliases in `short_aliases` list require word boundary matching (`\\b`)\n",
    "2. All other aliases use substring matching\n",
    "3. Case-insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "matcher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matcher test cases:\n",
      "  ✓ 'LeBron is washed' → ['LeBron James']\n",
      "  ✓ 'Curry cooking tonight' → ['Stephen Curry']\n",
      "  ✓ 'The curry was delicious' → ['Stephen Curry']\n",
      "  ✓ 'AD is questionable' → ['Anthony Davis']\n",
      "  ✓ 'That ad was annoying' → ['Anthony Davis']\n",
      "  ✓ 'Bron haters in shambles' → ['LeBron James']\n",
      "  ✓ 'LeGM at it again' → ['Aaron Gordon', 'LeBron James']\n",
      "  ✗ 'Great ball movement' → No match\n"
     ]
    }
   ],
   "source": [
    "def mentions_player(text: str) -> tuple[bool, list[str]]:\n",
    "    \"\"\"\n",
    "    Check if text mentions any tracked player.\n",
    "    \n",
    "    Returns:\n",
    "        (has_mention, list_of_players_found)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return False, []\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    found = []\n",
    "    \n",
    "    for player, aliases in PLAYER_ALIASES.items():\n",
    "        for alias in aliases:\n",
    "            alias_lower = alias.lower()\n",
    "            \n",
    "            # Check if this alias needs word boundary matching\n",
    "            if alias_lower in SHORT_ALIASES:\n",
    "                pattern = r'\\b' + re.escape(alias_lower) + r'\\b'\n",
    "                if re.search(pattern, text_lower):\n",
    "                    found.append(player)\n",
    "                    break\n",
    "            else:\n",
    "                # Standard substring matching\n",
    "                if alias_lower in text_lower:\n",
    "                    found.append(player)\n",
    "                    break\n",
    "    \n",
    "    return len(found) > 0, found\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"LeBron is washed\",\n",
    "    \"Curry cooking tonight\",\n",
    "    \"The curry was delicious\",  # Should NOT match (food)\n",
    "    \"AD is questionable\",       # Should match Anthony Davis\n",
    "    \"That ad was annoying\",     # Should NOT match (advertisement)\n",
    "    \"Bron haters in shambles\",\n",
    "    \"LeGM at it again\",\n",
    "    \"Great ball movement\",      # Should NOT match\n",
    "]\n",
    "\n",
    "print(\"Matcher test cases:\")\n",
    "for text in test_cases:\n",
    "    has_mention, players = mentions_player(text)\n",
    "    status = \"✓\" if has_mention else \"✗\"\n",
    "    print(f\"  {status} '{text}' → {players if players else 'No match'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-section",
   "metadata": {},
   "source": [
    "## 3. Sample Comments\n",
    "\n",
    "Load a random sample of 1,000 comments that pass the player-mention filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "count-total",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting comments (this may take a minute)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning: 6891163it [10:07, 11342.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total comments: 6,891,163\n",
      "Player mentions: 2,814,947 (40.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First pass: count total comments and player-mention comments\n",
    "total_comments = 0\n",
    "player_mention_count = 0\n",
    "\n",
    "print(\"Counting comments (this may take a minute)...\")\n",
    "with open(COMMENTS_PATH) as f:\n",
    "    for line in tqdm(f, desc=\"Scanning\"):\n",
    "        total_comments += 1\n",
    "        comment = json.loads(line)\n",
    "        if mentions_player(comment.get(\"body\", \"\"))[0]:\n",
    "            player_mention_count += 1\n",
    "\n",
    "pct = player_mention_count / total_comments * 100\n",
    "print(f\"\\nTotal comments: {total_comments:,}\")\n",
    "print(f\"Player mentions: {player_mention_count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sample-comments",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 1000 player-mention comments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 6891163it [10:12, 11252.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampled 1000 comments\n",
      "\n",
      "Sample comments:\n",
      "  [Nikola Jokic] Hate to say it but this was peak Chokic the last minute. Awful decision\n",
      "  [Jaylen Brown] i was eating either way but celtics blowing a big lead and jaylen brown missing ...\n",
      "  [Ben Simmons] The Benson family used to have Mickey Loomis running both the Pels *and* the Sai...\n",
      "  [Aaron Gordon] You should watch it again! I think you will be entertained for sure.\n",
      "  [Aaron Gordon] This is less about free agency and more about having the flexibility to trade fo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample 1,000 player-mention comments using reservoir sampling\n",
    "SAMPLE_SIZE = 1000\n",
    "random.seed(42)  # Reproducibility\n",
    "\n",
    "sample_comments = []\n",
    "seen = 0\n",
    "\n",
    "print(f\"Sampling {SAMPLE_SIZE} player-mention comments...\")\n",
    "with open(COMMENTS_PATH) as f:\n",
    "    for line in tqdm(f, desc=\"Sampling\"):\n",
    "        comment = json.loads(line)\n",
    "        body = comment.get(\"body\", \"\")\n",
    "        \n",
    "        has_mention, players = mentions_player(body)\n",
    "        if not has_mention:\n",
    "            continue\n",
    "        \n",
    "        seen += 1\n",
    "        \n",
    "        # Reservoir sampling\n",
    "        if len(sample_comments) < SAMPLE_SIZE:\n",
    "            sample_comments.append({\n",
    "                \"id\": comment.get(\"id\"),\n",
    "                \"body\": body,\n",
    "                \"players\": players,\n",
    "            })\n",
    "        else:\n",
    "            j = random.randint(0, seen - 1)\n",
    "            if j < SAMPLE_SIZE:\n",
    "                sample_comments[j] = {\n",
    "                    \"id\": comment.get(\"id\"),\n",
    "                    \"body\": body,\n",
    "                    \"players\": players,\n",
    "                }\n",
    "\n",
    "print(f\"\\nSampled {len(sample_comments)} comments\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample comments:\")\n",
    "for c in sample_comments[:5]:\n",
    "    body_preview = c['body'][:80] + \"...\" if len(c['body']) > 80 else c['body']\n",
    "    print(f\"  [{c['players'][0]}] {body_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "body-length-dist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment body length (characters):\n",
      "  Min: 4\n",
      "  Max: 1985\n",
      "  Mean: 167\n",
      "  Median: 106\n",
      "  P95: 533\n"
     ]
    }
   ],
   "source": [
    "# Analyze comment body length distribution\n",
    "body_lengths = [len(c[\"body\"]) for c in sample_comments]\n",
    "\n",
    "import statistics\n",
    "print(\"Comment body length (characters):\")\n",
    "print(f\"  Min: {min(body_lengths)}\")\n",
    "print(f\"  Max: {max(body_lengths)}\")\n",
    "print(f\"  Mean: {statistics.mean(body_lengths):.0f}\")\n",
    "print(f\"  Median: {statistics.median(body_lengths):.0f}\")\n",
    "print(f\"  P95: {sorted(body_lengths)[int(len(body_lengths) * 0.95)]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-section",
   "metadata": {},
   "source": [
    "## 4. Define Prompt Template\n",
    "\n",
    "This is the actual prompt we'll use for batch classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-template",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM PROMPT ===\n",
      "You analyze NBA basketball comments from Reddit and classify sentiment toward players.\n",
      "\n",
      "You must understand:\n",
      "- Sports slang inverts meaning: \"nasty\", \"disgusting\", \"filthy\", \"sick\" = POSITIVE (impressive play)\n",
      "- \"brick\" = missed shot (negative)\n",
      "- \"cooked\", \"washed\", \"fraud\" = negative\n",
      "- \"GOAT\" = greatest of all time (positive)\n",
      "- Sarcasm is extremely common in r/NBA\n",
      "- Nicknames: \"The King\", \"Bron\" = LeBron James; \"Chef Curry\", \"Steph\" = Stephen Curry\n",
      "\n",
      "Respond in JSON format:\n",
      "{{\n",
      "  \"sentiment\": \"positive\" | \"negative\" | \"neutral\",\n",
      "  \"confidence\": 0.0-1.0,\n",
      "  \"target_player\": \"Player Name\" | null,\n",
      "  \"reasoning\": \"Brief explanation\"\n",
      "}}\n",
      "\n",
      "\n",
      "=== USER MESSAGE (example) ===\n",
      "Analyze this r/NBA comment:\n",
      "\n",
      "Hate to say it but this was peak Chokic the last minute. Awful decision\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You analyze NBA basketball comments from Reddit and classify sentiment toward players.\n",
    "\n",
    "You must understand:\n",
    "- Sports slang inverts meaning: \"nasty\", \"disgusting\", \"filthy\", \"sick\" = POSITIVE (impressive play)\n",
    "- \"brick\" = missed shot (negative)\n",
    "- \"cooked\", \"washed\", \"fraud\" = negative\n",
    "- \"GOAT\" = greatest of all time (positive)\n",
    "- Sarcasm is extremely common in r/NBA\n",
    "- Nicknames: \"The King\", \"Bron\" = LeBron James; \"Chef Curry\", \"Steph\" = Stephen Curry\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\n",
    "  \"sentiment\": \"positive\" | \"negative\" | \"neutral\",\n",
    "  \"confidence\": 0.0-1.0,\n",
    "  \"target_player\": \"Player Name\" | null,\n",
    "  \"reasoning\": \"Brief explanation\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def build_user_message(comment_body: str) -> str:\n",
    "    \"\"\"Build the user message for a comment.\"\"\"\n",
    "    return f\"Analyze this r/NBA comment:\\n\\n{comment_body}\"\n",
    "\n",
    "# Show example\n",
    "example_comment = sample_comments[0][\"body\"]\n",
    "print(\"=== SYSTEM PROMPT ===\")\n",
    "print(SYSTEM_PROMPT)\n",
    "print(\"\\n=== USER MESSAGE (example) ===\")\n",
    "print(build_user_message(example_comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "token-section",
   "metadata": {},
   "source": [
    "## 5. Count Actual Tokens via API\n",
    "\n",
    "Use Anthropic's `count_tokens()` to get precise input token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "count-tokens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test comment tokens: 238\n",
      "Test comment length: 71 chars\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"claude-haiku-4-5-20251001\"\n",
    "\n",
    "def count_input_tokens(comment_body: str) -> int:\n",
    "    \"\"\"Count input tokens for a single comment using the API.\"\"\"\n",
    "    result = client.messages.count_tokens(\n",
    "        model=MODEL,\n",
    "        system=SYSTEM_PROMPT,\n",
    "        messages=[{\"role\": \"user\", \"content\": build_user_message(comment_body)}]\n",
    "    )\n",
    "    return result.input_tokens\n",
    "\n",
    "# Test on first comment\n",
    "test_tokens = count_input_tokens(sample_comments[0][\"body\"])\n",
    "print(f\"Test comment tokens: {test_tokens}\")\n",
    "print(f\"Test comment length: {len(sample_comments[0]['body'])} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "count-all-tokens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting tokens for all samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting tokens: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [09:00<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token count complete for 1000 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Count tokens for all sampled comments\n",
    "# Note: This makes 1,000 API calls but count_tokens is cheap/fast\n",
    "\n",
    "token_counts = []\n",
    "\n",
    "print(\"Counting tokens for all samples...\")\n",
    "for comment in tqdm(sample_comments, desc=\"Counting tokens\"):\n",
    "    tokens = count_input_tokens(comment[\"body\"])\n",
    "    token_counts.append(tokens)\n",
    "    comment[\"input_tokens\"] = tokens\n",
    "\n",
    "print(f\"\\nToken count complete for {len(token_counts)} comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "token-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token distribution:\n",
      "  Min: 220\n",
      "  Max: 670\n",
      "  Mean: 261.1\n",
      "  Median: 246.0\n",
      "  Std Dev: 49.6\n",
      "  P95: 343\n",
      "  P99: 500\n",
      "\n",
      "System prompt overhead: ~209 tokens\n"
     ]
    }
   ],
   "source": [
    "# Token distribution statistics\n",
    "print(\"Input token distribution:\")\n",
    "print(f\"  Min: {min(token_counts)}\")\n",
    "print(f\"  Max: {max(token_counts)}\")\n",
    "print(f\"  Mean: {statistics.mean(token_counts):.1f}\")\n",
    "print(f\"  Median: {statistics.median(token_counts):.1f}\")\n",
    "print(f\"  Std Dev: {statistics.stdev(token_counts):.1f}\")\n",
    "print(f\"  P95: {sorted(token_counts)[int(len(token_counts) * 0.95)]}\")\n",
    "print(f\"  P99: {sorted(token_counts)[int(len(token_counts) * 0.99)]}\")\n",
    "\n",
    "# System prompt overhead (constant)\n",
    "system_only = client.messages.count_tokens(\n",
    "    model=MODEL,\n",
    "    system=SYSTEM_PROMPT,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"x\"}]  # minimal user message\n",
    ").input_tokens\n",
    "print(f\"\\nSystem prompt overhead: ~{system_only - 1} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output-section",
   "metadata": {},
   "source": [
    "## 6. Estimate Output Tokens\n",
    "\n",
    "The response schema is structured. Let's estimate output token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "output-estimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output token estimates:\n",
      "  optimistic: 100 tokens\n",
      "  expected: 112 tokens\n",
      "  pessimistic: 136 tokens\n"
     ]
    }
   ],
   "source": [
    "# Expected output format:\n",
    "# {\n",
    "#   \"sentiment\": \"negative\",\n",
    "#   \"confidence\": 0.85,\n",
    "#   \"target_player\": \"LeBron James\",\n",
    "#   \"reasoning\": \"The comment uses 'washed' which is negative slang for a declining player.\"\n",
    "# }\n",
    "\n",
    "# Estimate based on typical responses:\n",
    "# - JSON structure: ~20 tokens\n",
    "# - sentiment + confidence: ~5 tokens\n",
    "# - target_player (avg name): ~5 tokens\n",
    "# - reasoning (1-2 sentences): ~20-40 tokens\n",
    "\n",
    "OUTPUT_TOKEN_ESTIMATES = {\n",
    "    \"optimistic\": 100,   # Short reasoning\n",
    "    \"expected\": 112,     # Typical reasoning\n",
    "    \"pessimistic\": 136,  # Longer reasoning\n",
    "}\n",
    "\n",
    "print(\"Output token estimates:\")\n",
    "for scenario, tokens in OUTPUT_TOKEN_ESTIMATES.items():\n",
    "    print(f\"  {scenario}: {tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "validate-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 1: 125 output tokens\n",
      "Comment 2: 136 output tokens\n",
      "Comment 3: 110 output tokens\n",
      "Comment 4: 99 output tokens\n",
      "Comment 5: 94 output tokens\n",
      "Comment 6: 101 output tokens\n",
      "Comment 7: 94 output tokens\n",
      "Comment 8: 135 output tokens\n",
      "Comment 9: 107 output tokens\n",
      "Comment 10: 126 output tokens\n",
      "\n",
      "Actual output tokens:\n",
      "  Mean: 112.7\n",
      "  Min: 94\n",
      "  Max: 136\n"
     ]
    }
   ],
   "source": [
    "# Optional: Run a few actual classifications to validate output estimates\n",
    "# Uncomment to run (costs a few cents)\n",
    "\n",
    "VALIDATE_OUTPUT = True  # Set to True to run validation\n",
    "\n",
    "if VALIDATE_OUTPUT:\n",
    "    actual_outputs = []\n",
    "    for comment in sample_comments[:10]:  # Just 10 samples\n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=150,\n",
    "            temperature=0.0,\n",
    "            system=SYSTEM_PROMPT,\n",
    "            messages=[{\"role\": \"user\", \"content\": build_user_message(comment[\"body\"])}]\n",
    "        )\n",
    "        actual_outputs.append(response.usage.output_tokens)\n",
    "        print(f\"Comment {len(actual_outputs)}: {response.usage.output_tokens} output tokens\")\n",
    "    \n",
    "    print(\"\\nActual output tokens:\")\n",
    "    print(f\"  Mean: {statistics.mean(actual_outputs):.1f}\")\n",
    "    print(f\"  Min: {min(actual_outputs)}\")\n",
    "    print(f\"  Max: {max(actual_outputs)}\")\n",
    "else:\n",
    "    print(\"Output validation skipped. Set VALIDATE_OUTPUT=True to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost-section",
   "metadata": {},
   "source": [
    "## 7. Calculate Costs\n",
    "\n",
    "**Batch API Pricing (Haiku 4.5):**\n",
    "- Input: $0.50 / MTok (50% off standard $1.00)\n",
    "- Output: $2.50 / MTok (50% off standard $5.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cost-calc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COST ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Mean input tokens per comment: 261.1\n",
      "Player-mention comments: 2,814,947\n",
      "\n",
      "--- Cost Scenarios ---\n",
      "\n",
      "OPTIMISTIC:\n",
      "  Output tokens: 100\n",
      "  Cost per 1K comments: $0.3805\n",
      "  Total cost (2,814,947 comments): $1071.23\n",
      "\n",
      "EXPECTED:\n",
      "  Output tokens: 112\n",
      "  Cost per 1K comments: $0.4105\n",
      "  Total cost (2,814,947 comments): $1155.68\n",
      "\n",
      "PESSIMISTIC:\n",
      "  Output tokens: 136\n",
      "  Cost per 1K comments: $0.4705\n",
      "  Total cost (2,814,947 comments): $1324.57\n"
     ]
    }
   ],
   "source": [
    "# Batch API pricing (per million tokens)\n",
    "BATCH_INPUT_PRICE = 0.50   # $/MTok\n",
    "BATCH_OUTPUT_PRICE = 2.50  # $/MTok\n",
    "\n",
    "# Calculate per-comment cost\n",
    "mean_input_tokens = statistics.mean(token_counts)\n",
    "\n",
    "def calculate_cost(input_tokens: float, output_tokens: float, count: int) -> float:\n",
    "    \"\"\"Calculate total cost for a batch of comments.\"\"\"\n",
    "    total_input = input_tokens * count\n",
    "    total_output = output_tokens * count\n",
    "    input_cost = (total_input / 1_000_000) * BATCH_INPUT_PRICE\n",
    "    output_cost = (total_output / 1_000_000) * BATCH_OUTPUT_PRICE\n",
    "    return input_cost + output_cost\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COST ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMean input tokens per comment: {mean_input_tokens:.1f}\")\n",
    "print(f\"Player-mention comments: {player_mention_count:,}\")\n",
    "\n",
    "print(\"\\n--- Cost Scenarios ---\")\n",
    "for scenario, output_tokens in OUTPUT_TOKEN_ESTIMATES.items():\n",
    "    total_cost = calculate_cost(mean_input_tokens, output_tokens, player_mention_count)\n",
    "    cost_per_comment = total_cost / player_mention_count * 1000  # per 1K comments\n",
    "    \n",
    "    print(f\"\\n{scenario.upper()}:\")\n",
    "    print(f\"  Output tokens: {output_tokens}\")\n",
    "    print(f\"  Cost per 1K comments: ${cost_per_comment:.4f}\")\n",
    "    print(f\"  Total cost ({player_mention_count:,} comments): ${total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "budget-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BUDGET ANALYSIS\n",
      "============================================================\n",
      "\n",
      "OPTIMISTIC (100 output tokens):\n",
      "  Cost per comment: $0.000381\n",
      "  Max comments at $200: 525,555\n",
      "  Max comments at $300: 788,332\n",
      "  ✗ Cannot classify all comments even at $300 ceiling\n",
      "    Would need to reduce by 2,026,615 comments (72.0%)\n",
      "\n",
      "EXPECTED (112 output tokens):\n",
      "  Cost per comment: $0.000411\n",
      "  Max comments at $200: 487,151\n",
      "  Max comments at $300: 730,727\n",
      "  ✗ Cannot classify all comments even at $300 ceiling\n",
      "    Would need to reduce by 2,084,220 comments (74.0%)\n",
      "\n",
      "PESSIMISTIC (136 output tokens):\n",
      "  Cost per comment: $0.000471\n",
      "  Max comments at $200: 425,034\n",
      "  Max comments at $300: 637,551\n",
      "  ✗ Cannot classify all comments even at $300 ceiling\n",
      "    Would need to reduce by 2,177,396 comments (77.4%)\n"
     ]
    }
   ],
   "source": [
    "# Budget-constrained analysis\n",
    "BUDGET_TARGET = 200\n",
    "BUDGET_CEILING = 300\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BUDGET ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for scenario, output_tokens in OUTPUT_TOKEN_ESTIMATES.items():\n",
    "    cost_per_comment = calculate_cost(mean_input_tokens, output_tokens, 1)\n",
    "    \n",
    "    max_at_target = int(BUDGET_TARGET / cost_per_comment)\n",
    "    max_at_ceiling = int(BUDGET_CEILING / cost_per_comment)\n",
    "    \n",
    "    print(f\"\\n{scenario.upper()} ({output_tokens} output tokens):\")\n",
    "    print(f\"  Cost per comment: ${cost_per_comment:.6f}\")\n",
    "    print(f\"  Max comments at ${BUDGET_TARGET}: {max_at_target:,}\")\n",
    "    print(f\"  Max comments at ${BUDGET_CEILING}: {max_at_ceiling:,}\")\n",
    "    \n",
    "    if max_at_target >= player_mention_count:\n",
    "        print(f\"  ✓ Can classify all {player_mention_count:,} comments within ${BUDGET_TARGET} target\")\n",
    "    elif max_at_ceiling >= player_mention_count:\n",
    "        print(f\"  ⚠ Can classify all comments, but exceeds ${BUDGET_TARGET} target\")\n",
    "    else:\n",
    "        shortfall = player_mention_count - max_at_ceiling\n",
    "        print(f\"  ✗ Cannot classify all comments even at ${BUDGET_CEILING} ceiling\")\n",
    "        print(f\"    Would need to reduce by {shortfall:,} comments ({shortfall/player_mention_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breakdown-section",
   "metadata": {},
   "source": [
    "## 8. Cost Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cost-breakdown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DETAILED COST BREAKDOWN (Expected Scenario)\n",
      "============================================================\n",
      "\n",
      "Comments to classify: 2,814,947\n",
      "\n",
      "Input Tokens:\n",
      "  Per comment: 261.1\n",
      "  Total: 734.98M tokens\n",
      "  Cost: $367.49\n",
      "\n",
      "Output Tokens:\n",
      "  Per comment: 112\n",
      "  Total: 315.27M tokens\n",
      "  Cost: $788.19\n",
      "\n",
      "──────────────────────────────\n",
      "TOTAL COST: $1155.68\n",
      "──────────────────────────────\n",
      "\n",
      "Input %: 31.8%\n",
      "Output %: 68.2%\n"
     ]
    }
   ],
   "source": [
    "# Detailed cost breakdown for expected scenario\n",
    "output_tokens = OUTPUT_TOKEN_ESTIMATES[\"expected\"]\n",
    "\n",
    "total_input_tokens = mean_input_tokens * player_mention_count\n",
    "total_output_tokens = output_tokens * player_mention_count\n",
    "\n",
    "input_cost = (total_input_tokens / 1_000_000) * BATCH_INPUT_PRICE\n",
    "output_cost = (total_output_tokens / 1_000_000) * BATCH_OUTPUT_PRICE\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED COST BREAKDOWN (Expected Scenario)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nComments to classify: {player_mention_count:,}\")\n",
    "print(\"\\nInput Tokens:\")\n",
    "print(f\"  Per comment: {mean_input_tokens:.1f}\")\n",
    "print(f\"  Total: {total_input_tokens/1_000_000:.2f}M tokens\")\n",
    "print(f\"  Cost: ${input_cost:.2f}\")\n",
    "print(\"\\nOutput Tokens:\")\n",
    "print(f\"  Per comment: {output_tokens}\")\n",
    "print(f\"  Total: {total_output_tokens/1_000_000:.2f}M tokens\")\n",
    "print(f\"  Cost: ${output_cost:.2f}\")\n",
    "print(f\"\\n{'─' * 30}\")\n",
    "print(f\"TOTAL COST: ${total_cost:.2f}\")\n",
    "print(f\"{'─' * 30}\")\n",
    "print(f\"\\nInput %: {input_cost/total_cost*100:.1f}%\")\n",
    "print(f\"Output %: {output_cost/total_cost*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter-audit-section",
   "metadata": {},
   "source": [
    "## 9. Filter Quality Audit\n",
    "\n",
    "Check for potential false positives in the player matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "false-positives",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential false positive patterns:\n",
      "(Comments where the match might not be about the player)\n",
      "\n",
      "'curry' (food reference?): 28 matches\n",
      "  → Curry THAT'S MY GOAT...\n",
      "  → The Rockets have rebuilt, peaked and rebuilt again all while Curry is still beating them. Sheesh wha...\n",
      "  → I was Mr H, Mr V chose jokic and curry b2b since its a snake draft...\n",
      "\n",
      "'ball' (basketball term?): 64 matches\n",
      "  → The Lakers ended up with possession of the ball, just that the challenge was on the out of bound cal...\n",
      "  → Writing was already on the wall by the time but I remember when I was getting into basketball and as...\n",
      "  → my 6 year old plays on 8' hoops with the 25.5 balls.   honestly the hoop feels a little tall, but th...\n",
      "\n",
      "'young' (age reference?): 14 matches\n",
      "  → I fear that the front office in ATL is gonna look at that run even in 2028 and say “see?! We almost ...\n",
      "  → Yeah, I think ppl don't realize there is some intriguing young talent on this team who have already ...\n",
      "  → This is the NBA's way of accelerating the next \"face of the league\" with all the older guys looking ...\n",
      "\n",
      "'smart' (adjective?): 7 matches\n",
      "  → Smart, Kennard, and BC are basically all having to be included to match a Butler salary....\n",
      "  → Yeah, mark had some wild decisions there late. I can’t believe he still challenged that shai foul as...\n",
      "  → Marcus Smart most playoff games...\n",
      "\n",
      "'green' (color?): 6 matches\n",
      "  → KD to the Lakers, LeBron to the Warriors, Kuminga and Green to PHX....\n",
      "  → Draymond Green...\n",
      "  → Beef \"Great Value Draymond Green\" Stew...\n",
      "\n",
      "'wall' (structure?): 5 matches\n",
      "  → Writing was already on the wall by the time but I remember when I was getting into basketball and as...\n",
      "  → YouTube got caught with their pants down on their NFL Sunday ticket deal and is now charging everyon...\n",
      "  → \"the NBA was watered down when Jordan played\" people when they realize the NBA has even more teams n...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Audit: Look for potential false positives\n",
    "print(\"Potential false positive patterns:\")\n",
    "print(\"(Comments where the match might not be about the player)\\n\")\n",
    "\n",
    "suspicious_patterns = [\n",
    "    (\"curry\", \"food reference?\"),\n",
    "    (\"ball\", \"basketball term?\"),\n",
    "    (\"young\", \"age reference?\"),\n",
    "    (\"smart\", \"adjective?\"),\n",
    "    (\"green\", \"color?\"),\n",
    "    (\"wall\", \"structure?\"),\n",
    "]\n",
    "\n",
    "for pattern, reason in suspicious_patterns:\n",
    "    matches = [c for c in sample_comments if pattern in c[\"body\"].lower()]\n",
    "    if matches:\n",
    "        print(f\"'{pattern}' ({reason}): {len(matches)} matches\")\n",
    "        for m in matches[:3]:\n",
    "            body_preview = m['body'][:100].replace('\\n', ' ')\n",
    "            print(f\"  → {body_preview}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "player-distribution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 mentioned players in sample:\n",
      "  Aaron Gordon: 316 (31.6%)\n",
      "  Brandon Ingram: 169 (16.9%)\n",
      "  LeBron James: 80 (8.0%)\n",
      "  Luka Doncic: 75 (7.5%)\n",
      "  Nikola Jokic: 60 (6.0%)\n",
      "  Stephen Curry: 52 (5.2%)\n",
      "  Tre Jones: 45 (4.5%)\n",
      "  Shai Gilgeous-Alexander: 44 (4.4%)\n",
      "  Giannis Antetokounmpo: 37 (3.7%)\n",
      "  Draymond Green: 32 (3.2%)\n",
      "  Kevin Durant: 29 (2.9%)\n",
      "  Jayson Tatum: 27 (2.7%)\n",
      "  Anthony Davis: 27 (2.7%)\n",
      "  Ben Simmons: 26 (2.6%)\n",
      "  Russell Westbrook: 24 (2.4%)\n",
      "  Anthony Edwards: 22 (2.2%)\n",
      "  Cade Cunningham: 21 (2.1%)\n",
      "  James Harden: 20 (2.0%)\n",
      "  Jimmy Butler: 20 (2.0%)\n",
      "  Victor Wembanyama: 18 (1.8%)\n"
     ]
    }
   ],
   "source": [
    "# Player mention distribution in sample\n",
    "from collections import Counter\n",
    "\n",
    "player_counts = Counter()\n",
    "for c in sample_comments:\n",
    "    for player in c[\"players\"]:\n",
    "        player_counts[player] += 1\n",
    "\n",
    "print(\"Top 20 mentioned players in sample:\")\n",
    "for player, count in player_counts.most_common(20):\n",
    "    pct = count / len(sample_comments) * 100\n",
    "    print(f\"  {player}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recommendations-section",
   "metadata": {},
   "source": [
    "## 10. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "recommendations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "Expected cost: $1155.68\n",
      "Pessimistic cost: $1324.57\n",
      "Budget target: $200\n",
      "Budget ceiling: $300\n",
      "\n",
      "❌ NO-GO: Expected cost exceeds budget ceiling.\n",
      "   Need to reduce to ~487,151 comments to hit $200\n",
      "   Options:\n",
      "   1. Clean up player config (remove redundant/noisy aliases)\n",
      "   2. Add high-false-positive aliases to exclusion list\n",
      "   3. Random sample subset of comments\n"
     ]
    }
   ],
   "source": [
    "# Generate go/no-go recommendation\n",
    "expected_cost = calculate_cost(mean_input_tokens, OUTPUT_TOKEN_ESTIMATES[\"expected\"], player_mention_count)\n",
    "pessimistic_cost = calculate_cost(mean_input_tokens, OUTPUT_TOKEN_ESTIMATES[\"pessimistic\"], player_mention_count)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nExpected cost: ${expected_cost:.2f}\")\n",
    "print(f\"Pessimistic cost: ${pessimistic_cost:.2f}\")\n",
    "print(f\"Budget target: ${BUDGET_TARGET}\")\n",
    "print(f\"Budget ceiling: ${BUDGET_CEILING}\")\n",
    "\n",
    "if expected_cost <= BUDGET_TARGET:\n",
    "    print(\"\\n✅ GO: Expected cost within budget target.\")\n",
    "    print(\"   Proceed to Phase 4 batch processing.\")\n",
    "elif expected_cost <= BUDGET_CEILING:\n",
    "    print(\"\\n⚠️  CONDITIONAL GO: Expected cost exceeds target but within ceiling.\")\n",
    "    print(\"   Options:\")\n",
    "    print(\"   1. Accept ~${:.0f} overrun\".format(expected_cost - BUDGET_TARGET))\n",
    "    print(\"   2. Tighten filter to reduce comment count\")\n",
    "    print(\"   3. Random sample to fit budget\")\n",
    "else:\n",
    "    print(\"\\n❌ NO-GO: Expected cost exceeds budget ceiling.\")\n",
    "    target_comments = int(BUDGET_TARGET / (expected_cost / player_mention_count))\n",
    "    print(f\"   Need to reduce to ~{target_comments:,} comments to hit ${BUDGET_TARGET}\")\n",
    "    print(\"   Options:\")\n",
    "    print(\"   1. Clean up player config (remove redundant/noisy aliases)\")\n",
    "    print(\"   2. Add high-false-positive aliases to exclusion list\")\n",
    "    print(\"   3. Random sample subset of comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY FOR PM CHECKPOINT\n",
      "============================================================\n",
      "\n",
      "Data:\n",
      "  Total cleaned comments: 6,891,163\n",
      "  Player-mention comments: 2,814,947 (40.8%)\n",
      "\n",
      "Token Metrics (from 1000 sample):\n",
      "  Mean input tokens: 261.1\n",
      "  Median input tokens: 246.0\n",
      "  P95 input tokens: 343\n",
      "  Estimated output tokens: 112\n",
      "\n",
      "Cost Projections:\n",
      "  Expected: $1155.68\n",
      "  Pessimistic: $1324.57\n",
      "  Budget target: $200\n",
      "  Budget ceiling: $300\n",
      "\n",
      "Filter Quality:\n",
      "  Players tracked: 92\n",
      "  Short aliases (word boundary): 36\n",
      "  [Review false positive audit above]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary for PM checkpoint\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY FOR PM CHECKPOINT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Data:\n",
    "  Total cleaned comments: {total_comments:,}\n",
    "  Player-mention comments: {player_mention_count:,} ({player_mention_count/total_comments*100:.1f}%)\n",
    "\n",
    "Token Metrics (from {len(sample_comments)} sample):\n",
    "  Mean input tokens: {mean_input_tokens:.1f}\n",
    "  Median input tokens: {statistics.median(token_counts):.1f}\n",
    "  P95 input tokens: {sorted(token_counts)[int(len(token_counts) * 0.95)]}\n",
    "  Estimated output tokens: {OUTPUT_TOKEN_ESTIMATES['expected']}\n",
    "\n",
    "Cost Projections:\n",
    "  Expected: ${expected_cost:.2f}\n",
    "  Pessimistic: ${pessimistic_cost:.2f}\n",
    "  Budget target: ${BUDGET_TARGET}\n",
    "  Budget ceiling: ${BUDGET_CEILING}\n",
    "\n",
    "Filter Quality:\n",
    "  Players tracked: {len(PLAYER_ALIASES)}\n",
    "  Short aliases (word boundary): {len(SHORT_ALIASES)}\n",
    "  [Review false positive audit above]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec22206",
   "metadata": {},
   "source": [
    "## Minimal Prompt Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "404a3f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MINIMAL PROMPT ===\n",
      "Classify sentiment toward NBA players. \n",
      "Slang: nasty/sick/filthy=positive, washed/brick/fraud/cooked=negative, GOAT=positive.\n",
      "\n",
      "Comment: Hate to say it but this was peak Chokic the last minute. Awful decision\n",
      "\n",
      "Respond ONLY with JSON: {\"s\":\"pos|neg|neu\",\"c\":0.0-1.0,\"p\":\"Player Name\"|null}\n",
      "\n",
      "Prompt length: 287 chars\n"
     ]
    }
   ],
   "source": [
    "# Minimal prompt - everything in user message, no system prompt\n",
    "def build_minimal_prompt(comment_body: str) -> str:\n",
    "    \"\"\"Build minimal user message for classification.\"\"\"\n",
    "    return f\"\"\"Classify sentiment toward NBA players. \n",
    "Slang: nasty/sick/filthy=positive, washed/brick/fraud/cooked=negative, GOAT=positive.\n",
    "\n",
    "Comment: {comment_body}\n",
    "\n",
    "Respond ONLY with JSON: {{\"s\":\"pos|neg|neu\",\"c\":0.0-1.0,\"p\":\"Player Name\"|null}}\"\"\"\n",
    "\n",
    "# Test\n",
    "example = sample_comments[0][\"body\"]\n",
    "print(\"=== MINIMAL PROMPT ===\")\n",
    "print(build_minimal_prompt(example))\n",
    "print(f\"\\nPrompt length: {len(build_minimal_prompt(example))} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0b3f0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting tokens for minimal prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [09:00<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimal prompt token distribution:\n",
      "  Min: 87\n",
      "  Max: 537\n",
      "  Mean: 128.1\n",
      "  Median: 113.0\n",
      "  P95: 210\n",
      "\n",
      "Reduction from original:\n",
      "  Original mean: 261.1 tokens\n",
      "  Minimal mean: 128.1 tokens\n",
      "  Savings: 133.0 tokens (50.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Count tokens for minimal prompt (no system prompt)\n",
    "def count_minimal_tokens(comment_body: str) -> int:\n",
    "    \"\"\"Count input tokens for minimal prompt.\"\"\"\n",
    "    return client.messages.count_tokens(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": build_minimal_prompt(comment_body)}]\n",
    "    ).input_tokens\n",
    "\n",
    "# Test on sample\n",
    "minimal_token_counts = []\n",
    "print(\"Counting tokens for minimal prompt...\")\n",
    "for comment in tqdm(sample_comments, desc=\"Counting\"):\n",
    "    tokens = count_minimal_tokens(comment[\"body\"])\n",
    "    minimal_token_counts.append(tokens)\n",
    "\n",
    "print(\"\\nMinimal prompt token distribution:\")\n",
    "print(f\"  Min: {min(minimal_token_counts)}\")\n",
    "print(f\"  Max: {max(minimal_token_counts)}\")\n",
    "print(f\"  Mean: {statistics.mean(minimal_token_counts):.1f}\")\n",
    "print(f\"  Median: {statistics.median(minimal_token_counts):.1f}\")\n",
    "print(f\"  P95: {sorted(minimal_token_counts)[int(len(minimal_token_counts) * 0.95)]}\")\n",
    "\n",
    "print(\"\\nReduction from original:\")\n",
    "print(f\"  Original mean: {mean_input_tokens:.1f} tokens\")\n",
    "print(f\"  Minimal mean: {statistics.mean(minimal_token_counts):.1f} tokens\")\n",
    "print(f\"  Savings: {mean_input_tokens - statistics.mean(minimal_token_counts):.1f} tokens ({(1 - statistics.mean(minimal_token_counts)/mean_input_tokens)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f28b1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing minimal prompt output...\n",
      "  Output: 29 tokens | ```json\n",
      "{\"s\":\"neg\",\"c\":0.85,\"p\":\"Nikola Jokic\"}\n",
      "```\n",
      "  Output: 23 tokens | ```json\n",
      "{\"s\":\"neu\",\"c\":0.5,\"p\":null}\n",
      "```\n",
      "  Output: 23 tokens | ```json\n",
      "{\"s\":\"neu\",\"c\":0.0,\"p\":null}\n",
      "```\n",
      "  Output: 23 tokens | ```json\n",
      "{\"s\":\"neu\",\"c\":0.5,\"p\":null}\n",
      "```\n",
      "  Output: 23 tokens | ```json\n",
      "{\"s\":\"neu\",\"c\":0.5,\"p\":null}\n",
      "```\n",
      "  Output: 23 tokens | ```json\n",
      "{\"s\":\"neu\",\"c\":0.5,\"p\":null}\n",
      "```\n",
      "  Output: 23 tokens | ```json\n",
      "{\"s\":\"neu\",\"c\":0.0,\"p\":null}\n",
      "```\n",
      "  Output: 23 tokens | ```json\n",
      "{\"s\":\"neu\",\"c\":0.5,\"p\":null}\n",
      "```\n",
      "  Output: 23 tokens | ```json\n",
      "{\"s\":\"neu\",\"c\":0.5,\"p\":null}\n",
      "```\n",
      "  Output: 26 tokens | ```json\n",
      "{\"s\":\"neg\",\"c\":0.9,\"p\":\"Jokic\"}\n",
      "```\n",
      "\n",
      "Actual minimal output tokens:\n",
      "  Mean: 23.9\n",
      "  Min: 23\n",
      "  Max: 29\n",
      "\n",
      "Using 23 tokens for output estimate\n"
     ]
    }
   ],
   "source": [
    "# Test actual output with minimal prompt\n",
    "MINIMAL_OUTPUT_TOKENS = 25  # Initial estimate for {\"s\":\"neg\",\"c\":0.85,\"p\":\"LeBron James\"}\n",
    "\n",
    "print(\"Testing minimal prompt output...\")\n",
    "minimal_outputs = []\n",
    "for comment in sample_comments[:10]:\n",
    "    response = client.messages.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=50,  # Constrain output\n",
    "        temperature=0.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": build_minimal_prompt(comment[\"body\"])}]\n",
    "    )\n",
    "    minimal_outputs.append(response.usage.output_tokens)\n",
    "    print(f\"  Output: {response.usage.output_tokens} tokens | {response.content[0].text}\")\n",
    "\n",
    "print(\"\\nActual minimal output tokens:\")\n",
    "print(f\"  Mean: {statistics.mean(minimal_outputs):.1f}\")\n",
    "print(f\"  Min: {min(minimal_outputs)}\")\n",
    "print(f\"  Max: {max(minimal_outputs)}\")\n",
    "\n",
    "# Update estimate based on actuals\n",
    "MINIMAL_OUTPUT_ESTIMATE = int(statistics.mean(minimal_outputs))\n",
    "print(f\"\\nUsing {MINIMAL_OUTPUT_ESTIMATE} tokens for output estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "279738ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Players tracked (v2): 92\n",
      "Short aliases (v2): 41\n",
      "\n",
      "Counting player mentions with cleaned config...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning: 6891163it [10:42, 10731.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Player mentions (v1): 2,814,947\n",
      "Player mentions (v2): 1,939,268\n",
      "Reduction: 875,679 (31.1%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Reload cleaned config\n",
    "with open(PLAYERS_CONFIG) as f:\n",
    "    player_config_v2 = yaml.safe_load(f)\n",
    "\n",
    "PLAYER_ALIASES_V2 = player_config_v2[\"players\"]\n",
    "SHORT_ALIASES_V2 = set(player_config_v2.get(\"short_aliases\", []))\n",
    "\n",
    "print(f\"Players tracked (v2): {len(PLAYER_ALIASES_V2)}\")\n",
    "print(f\"Short aliases (v2): {len(SHORT_ALIASES_V2)}\")\n",
    "\n",
    "# Update matcher to use v2 config\n",
    "def mentions_player_v2(text: str) -> tuple[bool, list[str]]:\n",
    "    \"\"\"Player matcher with cleaned config.\"\"\"\n",
    "    if not text:\n",
    "        return False, []\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    found = []\n",
    "    \n",
    "    for player, aliases in PLAYER_ALIASES_V2.items():\n",
    "        for alias in aliases:\n",
    "            alias_lower = alias.lower()\n",
    "            \n",
    "            if alias_lower in SHORT_ALIASES_V2:\n",
    "                pattern = r'\\b' + re.escape(alias_lower) + r'\\b'\n",
    "                if re.search(pattern, text_lower):\n",
    "                    found.append(player)\n",
    "                    break\n",
    "            else:\n",
    "                if alias_lower in text_lower:\n",
    "                    found.append(player)\n",
    "                    break\n",
    "    \n",
    "    return len(found) > 0, found\n",
    "\n",
    "# Re-count player mentions with cleaned config\n",
    "player_mention_count_v2 = 0\n",
    "print(\"\\nCounting player mentions with cleaned config...\")\n",
    "with open(COMMENTS_PATH) as f:\n",
    "    for line in tqdm(f, desc=\"Scanning\"):\n",
    "        comment = json.loads(line)\n",
    "        if mentions_player_v2(comment.get(\"body\", \"\"))[0]:\n",
    "            player_mention_count_v2 += 1\n",
    "\n",
    "print(f\"\\nPlayer mentions (v1): {player_mention_count:,}\")\n",
    "print(f\"Player mentions (v2): {player_mention_count_v2:,}\")\n",
    "print(f\"Reduction: {player_mention_count - player_mention_count_v2:,} ({(1 - player_mention_count_v2/player_mention_count)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df258859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL COST PROJECTION (Minimal Prompt + Cleaned Filter)\n",
      "============================================================\n",
      "\n",
      "Comments to classify: 1,939,268\n",
      "\n",
      "Input Tokens:\n",
      "  Per comment: 128.1\n",
      "  Total: 248.45M\n",
      "  Cost: $124.22\n",
      "\n",
      "Output Tokens:\n",
      "  Per comment: 23\n",
      "  Total: 44.60M\n",
      "  Cost: $111.51\n",
      "\n",
      "──────────────────────────────\n",
      "TOTAL COST: $235.73\n",
      "──────────────────────────────\n",
      "\n",
      "Budget target: $200\n",
      "Budget ceiling: $300\n",
      "\n",
      "⚠️ CONDITIONAL GO: $235.73 exceeds target but within ceiling\n"
     ]
    }
   ],
   "source": [
    "# Final cost calculation with minimal prompt + cleaned filter\n",
    "minimal_input_mean = statistics.mean(minimal_token_counts)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL COST PROJECTION (Minimal Prompt + Cleaned Filter)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate costs\n",
    "total_input = minimal_input_mean * player_mention_count_v2\n",
    "total_output = MINIMAL_OUTPUT_ESTIMATE * player_mention_count_v2\n",
    "\n",
    "input_cost = (total_input / 1_000_000) * BATCH_INPUT_PRICE\n",
    "output_cost = (total_output / 1_000_000) * BATCH_OUTPUT_PRICE\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "print(f\"\\nComments to classify: {player_mention_count_v2:,}\")\n",
    "print(\"\\nInput Tokens:\")\n",
    "print(f\"  Per comment: {minimal_input_mean:.1f}\")\n",
    "print(f\"  Total: {total_input/1_000_000:.2f}M\")\n",
    "print(f\"  Cost: ${input_cost:.2f}\")\n",
    "print(\"\\nOutput Tokens:\")\n",
    "print(f\"  Per comment: {MINIMAL_OUTPUT_ESTIMATE}\")\n",
    "print(f\"  Total: {total_output/1_000_000:.2f}M\")\n",
    "print(f\"  Cost: ${output_cost:.2f}\")\n",
    "print(f\"\\n{'─' * 30}\")\n",
    "print(f\"TOTAL COST: ${total_cost:.2f}\")\n",
    "print(f\"{'─' * 30}\")\n",
    "\n",
    "# Budget check\n",
    "print(f\"\\nBudget target: ${BUDGET_TARGET}\")\n",
    "print(f\"Budget ceiling: ${BUDGET_CEILING}\")\n",
    "\n",
    "if total_cost <= BUDGET_TARGET:\n",
    "    print(f\"\\n✅ GO: ${total_cost:.2f} is within ${BUDGET_TARGET} target!\")\n",
    "elif total_cost <= BUDGET_CEILING:\n",
    "    print(f\"\\n⚠️ CONDITIONAL GO: ${total_cost:.2f} exceeds target but within ceiling\")\n",
    "else:\n",
    "    print(\"\\n❌ Still over budget. Need further reduction.\")\n",
    "    max_comments = int(BUDGET_TARGET / (total_cost / player_mention_count_v2))\n",
    "    print(f\"   Max comments at ${BUDGET_TARGET}: {max_comments:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1d7455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTIMIZATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "                        BEFORE          AFTER           CHANGE\n",
      "─────────────────────────────────────────────────────────────────\n",
      "Input tokens/comment    261             128              -51%\n",
      "Output tokens/comment   112             23               -79%\n",
      "Player-mention comments 2,814,947       1,939,268         -31%\n",
      "─────────────────────────────────────────────────────────────────\n",
      "Total cost              $1155.68         $235.73           -80%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Before/After comparison\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTIMIZATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "                        BEFORE          AFTER           CHANGE\n",
    "─────────────────────────────────────────────────────────────────\n",
    "Input tokens/comment    {mean_input_tokens:.0f}             {minimal_input_mean:.0f}              {((minimal_input_mean/mean_input_tokens)-1)*100:+.0f}%\n",
    "Output tokens/comment   {OUTPUT_TOKEN_ESTIMATES['expected']}             {MINIMAL_OUTPUT_ESTIMATE}               {((MINIMAL_OUTPUT_ESTIMATE/OUTPUT_TOKEN_ESTIMATES['expected'])-1)*100:+.0f}%\n",
    "Player-mention comments {player_mention_count:,}       {player_mention_count_v2:,}         {((player_mention_count_v2/player_mention_count)-1)*100:+.0f}%\n",
    "─────────────────────────────────────────────────────────────────\n",
    "Total cost              ${calculate_cost(mean_input_tokens, OUTPUT_TOKEN_ESTIMATES['expected'], player_mention_count):.2f}         ${total_cost:.2f}           {((total_cost/calculate_cost(mean_input_tokens, OUTPUT_TOKEN_ESTIMATES['expected'], player_mention_count))-1)*100:+.0f}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22adb916",
   "metadata": {},
   "source": [
    "## Accuracy Validation: Minimal vs Original Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ffe4cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: 23 test cases\n",
      "  Positive: 8\n",
      "  Negative: 12\n",
      "  Neutral: 3\n"
     ]
    }
   ],
   "source": [
    "# Ground truth test cases (from notebook 01 + new edge cases)\n",
    "VALIDATION_SET = [\n",
    "    # Clear positive\n",
    "    {\"text\": \"LeBron is the GOAT\", \"expected\": \"pos\", \"player\": \"LeBron James\"},\n",
    "    {\"text\": \"Curry is absolutely disgusting from three\", \"expected\": \"pos\", \"player\": \"Stephen Curry\"},\n",
    "    {\"text\": \"That dunk was NASTY\", \"expected\": \"pos\", \"player\": None},\n",
    "    {\"text\": \"Jokic is a generational talent\", \"expected\": \"pos\", \"player\": \"Nikola Jokic\"},\n",
    "    {\"text\": \"Shai is so good it's unfair\", \"expected\": \"pos\", \"player\": \"Shai Gilgeous-Alexander\"},\n",
    "    \n",
    "    # Clear negative\n",
    "    {\"text\": \"Westbrook is absolute trash\", \"expected\": \"neg\", \"player\": \"Russell Westbrook\"},\n",
    "    {\"text\": \"LeBron is washed\", \"expected\": \"neg\", \"player\": \"LeBron James\"},\n",
    "    {\"text\": \"Harden choked again in the playoffs\", \"expected\": \"neg\", \"player\": \"James Harden\"},\n",
    "    {\"text\": \"Another brick from Westbrick\", \"expected\": \"neg\", \"player\": \"Russell Westbrook\"},\n",
    "    {\"text\": \"Ben Simmons is a fraud\", \"expected\": \"neg\", \"player\": \"Ben Simmons\"},\n",
    "    {\"text\": \"KD took the easy way out\", \"expected\": \"neg\", \"player\": \"Kevin Durant\"},\n",
    "    \n",
    "    # Neutral\n",
    "    {\"text\": \"Lakers play tomorrow at 7pm\", \"expected\": \"neu\", \"player\": None},\n",
    "    {\"text\": \"Tatum had 25 points last night\", \"expected\": \"neu\", \"player\": \"Jayson Tatum\"},\n",
    "    {\"text\": \"Trade deadline is next week\", \"expected\": \"neu\", \"player\": None},\n",
    "    \n",
    "    # Tricky - slang that inverts meaning\n",
    "    {\"text\": \"Ant is fucking sick bro\", \"expected\": \"pos\", \"player\": \"Anthony Edwards\"},\n",
    "    {\"text\": \"Giannis is a freak of nature\", \"expected\": \"pos\", \"player\": \"Giannis Antetokounmpo\"},\n",
    "    {\"text\": \"Luka cooked the entire defense\", \"expected\": \"pos\", \"player\": \"Luka Doncic\"},  # \"cooked\" = dominated (positive)\n",
    "    {\"text\": \"Dame got cooked on defense\", \"expected\": \"neg\", \"player\": \"Damian Lillard\"},  # \"got cooked\" = exposed (negative)\n",
    "    \n",
    "    # Sarcasm (hardest)\n",
    "    {\"text\": \"Wow Simmons shot a three, league fucked\", \"expected\": \"neg\", \"player\": \"Ben Simmons\"},\n",
    "    {\"text\": \"Great defense from Trae Young as usual\", \"expected\": \"neg\", \"player\": \"Trae Young\"},\n",
    "    \n",
    "    # Negative nicknames\n",
    "    {\"text\": \"LeMickey needs more help\", \"expected\": \"neg\", \"player\": \"LeBron James\"},\n",
    "    {\"text\": \"Westbrick back at it\", \"expected\": \"neg\", \"player\": \"Russell Westbrook\"},\n",
    "    {\"text\": \"ADisney only shows up in the bubble\", \"expected\": \"neg\", \"player\": \"Anthony Davis\"},\n",
    "]\n",
    "\n",
    "print(f\"Validation set: {len(VALIDATION_SET)} test cases\")\n",
    "print(f\"  Positive: {sum(1 for t in VALIDATION_SET if t['expected'] == 'pos')}\")\n",
    "print(f\"  Negative: {sum(1 for t in VALIDATION_SET if t['expected'] == 'neg')}\")\n",
    "print(f\"  Neutral: {sum(1 for t in VALIDATION_SET if t['expected'] == 'neu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10d46001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation (this will take a minute)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:49<00:00,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json as json_lib\n",
    "\n",
    "def parse_minimal_response(text: str) -> dict:\n",
    "    \"\"\"Parse minimal JSON response.\"\"\"\n",
    "    try:\n",
    "        # Handle potential markdown wrapping\n",
    "        clean = text.strip()\n",
    "        if clean.startswith(\"```\"):\n",
    "            clean = clean.split(\"```\")[1]\n",
    "            if clean.startswith(\"json\"):\n",
    "                clean = clean[4:]\n",
    "        return json_lib.loads(clean)\n",
    "    except:\n",
    "        return {\"s\": \"error\", \"c\": 0, \"p\": None}\n",
    "\n",
    "def parse_original_response(text: str) -> dict:\n",
    "    \"\"\"Parse original JSON response.\"\"\"\n",
    "    try:\n",
    "        clean = text.strip()\n",
    "        if clean.startswith(\"```\"):\n",
    "            clean = clean.split(\"```\")[1]\n",
    "            if clean.startswith(\"json\"):\n",
    "                clean = clean[4:]\n",
    "        data = json_lib.loads(clean)\n",
    "        # Normalize to minimal format\n",
    "        sentiment_map = {\"positive\": \"pos\", \"negative\": \"neg\", \"neutral\": \"neu\"}\n",
    "        return {\n",
    "            \"s\": sentiment_map.get(data.get(\"sentiment\", \"\"), data.get(\"sentiment\", \"\")[:3]),\n",
    "            \"c\": data.get(\"confidence\", 0),\n",
    "            \"p\": data.get(\"target_player\")\n",
    "        }\n",
    "    except:\n",
    "        return {\"s\": \"error\", \"c\": 0, \"p\": None}\n",
    "\n",
    "# Run validation\n",
    "results = []\n",
    "\n",
    "print(\"Running validation (this will take a minute)...\\n\")\n",
    "for i, test in enumerate(tqdm(VALIDATION_SET, desc=\"Validating\")):\n",
    "    # Minimal prompt\n",
    "    minimal_resp = client.messages.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=50,\n",
    "        temperature=0.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": build_minimal_prompt(test[\"text\"])}]\n",
    "    )\n",
    "    minimal_parsed = parse_minimal_response(minimal_resp.content[0].text)\n",
    "    \n",
    "    # Original prompt (for comparison)\n",
    "    original_resp = client.messages.create(\n",
    "        model=MODEL,\n",
    "        max_tokens=150,\n",
    "        temperature=0.0,\n",
    "        system=SYSTEM_PROMPT,\n",
    "        messages=[{\"role\": \"user\", \"content\": build_user_message(test[\"text\"])}]\n",
    "    )\n",
    "    original_parsed = parse_original_response(original_resp.content[0].text)\n",
    "    \n",
    "    results.append({\n",
    "        \"text\": test[\"text\"],\n",
    "        \"expected\": test[\"expected\"],\n",
    "        \"expected_player\": test[\"player\"],\n",
    "        \"minimal\": minimal_parsed,\n",
    "        \"original\": original_parsed,\n",
    "    })\n",
    "\n",
    "print(\"Validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b6447b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ACCURACY COMPARISON\n",
      "============================================================\n",
      "\n",
      "Original prompt accuracy: 95.7%\n",
      "Minimal prompt accuracy:  95.7%\n",
      "Prompt agreement:         100.0%\n",
      "\n",
      "--- Accuracy by Sentiment ---\n",
      "  pos: Original 100% | Minimal 100%\n",
      "  neg: Original 92% | Minimal 92%\n",
      "  neu: Original 100% | Minimal 100%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy metrics\n",
    "def calc_accuracy(results, prompt_key):\n",
    "    correct = sum(1 for r in results if r[prompt_key][\"s\"] == r[\"expected\"])\n",
    "    return correct / len(results) * 100\n",
    "\n",
    "minimal_accuracy = calc_accuracy(results, \"minimal\")\n",
    "original_accuracy = calc_accuracy(results, \"original\")\n",
    "\n",
    "# Agreement between prompts\n",
    "agreement = sum(1 for r in results if r[\"minimal\"][\"s\"] == r[\"original\"][\"s\"])\n",
    "agreement_pct = agreement / len(results) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ACCURACY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal prompt accuracy: {original_accuracy:.1f}%\")\n",
    "print(f\"Minimal prompt accuracy:  {minimal_accuracy:.1f}%\")\n",
    "print(f\"Prompt agreement:         {agreement_pct:.1f}%\")\n",
    "\n",
    "# Breakdown by category\n",
    "print(\"\\n--- Accuracy by Sentiment ---\")\n",
    "for sentiment in [\"pos\", \"neg\", \"neu\"]:\n",
    "    subset = [r for r in results if r[\"expected\"] == sentiment]\n",
    "    if subset:\n",
    "        orig_acc = sum(1 for r in subset if r[\"original\"][\"s\"] == sentiment) / len(subset) * 100\n",
    "        min_acc = sum(1 for r in subset if r[\"minimal\"][\"s\"] == sentiment) / len(subset) * 100\n",
    "        print(f\"  {sentiment}: Original {orig_acc:.0f}% | Minimal {min_acc:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b94b6f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MINIMAL PROMPT ERRORS\n",
      "============================================================\n",
      "\n",
      "Total errors: 1/23\n",
      "\n",
      "  Text: \"Wow Simmons shot a three, league fucked\"\n",
      "  Expected: neg | Minimal: pos | Original: pos\n",
      "\n",
      "============================================================\n",
      "PROMPT DISAGREEMENTS (Minimal ≠ Original)\n",
      "============================================================\n",
      "\n",
      "Total disagreements: 0/23\n"
     ]
    }
   ],
   "source": [
    "# Show cases where minimal differs from expected\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MINIMAL PROMPT ERRORS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "errors = [r for r in results if r[\"minimal\"][\"s\"] != r[\"expected\"]]\n",
    "print(f\"\\nTotal errors: {len(errors)}/{len(results)}\")\n",
    "\n",
    "for r in errors:\n",
    "    print(f\"\\n  Text: \\\"{r['text'][:60]}...\\\"\" if len(r['text']) > 60 else f\"\\n  Text: \\\"{r['text']}\\\"\")\n",
    "    print(f\"  Expected: {r['expected']} | Minimal: {r['minimal']['s']} | Original: {r['original']['s']}\")\n",
    "\n",
    "# Show disagreements between prompts\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROMPT DISAGREEMENTS (Minimal ≠ Original)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "disagreements = [r for r in results if r[\"minimal\"][\"s\"] != r[\"original\"][\"s\"]]\n",
    "print(f\"\\nTotal disagreements: {len(disagreements)}/{len(results)}\")\n",
    "\n",
    "for r in disagreements:\n",
    "    print(f\"\\n  Text: \\\"{r['text'][:60]}\\\"\")\n",
    "    print(f\"  Expected: {r['expected']} | Minimal: {r['minimal']['s']} | Original: {r['original']['s']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eda0ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLAYER ATTRIBUTION ACCURACY\n",
      "============================================================\n",
      "\n",
      "Original prompt: 100.0% (23/23)\n",
      "Minimal prompt:  95.7% (22/23)\n",
      "\n",
      "--- Minimal Prompt Player Errors ---\n",
      "Total: 1/23\n",
      "\n",
      "  Text: \"ADisney only shows up in the bubble\"\n",
      "  Expected: Anthony Davis\n",
      "  Minimal:  AD\n",
      "  Original: Anthony Davis\n"
     ]
    }
   ],
   "source": [
    "def normalize_player(name):\n",
    "    \"\"\"Normalize player name for comparison.\"\"\"\n",
    "    if name is None:\n",
    "        return None\n",
    "    return name.lower().strip()\n",
    "\n",
    "def player_match(predicted, expected):\n",
    "    \"\"\"Check if predicted player matches expected.\"\"\"\n",
    "    pred_norm = normalize_player(predicted)\n",
    "    exp_norm = normalize_player(expected)\n",
    "    \n",
    "    # Both None = correct\n",
    "    if pred_norm is None and exp_norm is None:\n",
    "        return True\n",
    "    # One None, one not = incorrect\n",
    "    if pred_norm is None or exp_norm is None:\n",
    "        return False\n",
    "    # Check if one contains the other (handles \"LeBron\" vs \"LeBron James\")\n",
    "    return pred_norm in exp_norm or exp_norm in pred_norm\n",
    "\n",
    "# Calculate player attribution accuracy\n",
    "minimal_player_correct = sum(1 for r in results if player_match(r[\"minimal\"][\"p\"], r[\"expected_player\"]))\n",
    "original_player_correct = sum(1 for r in results if player_match(r[\"original\"][\"p\"], r[\"expected_player\"]))\n",
    "\n",
    "minimal_player_acc = minimal_player_correct / len(results) * 100\n",
    "original_player_acc = original_player_correct / len(results) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PLAYER ATTRIBUTION ACCURACY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal prompt: {original_player_acc:.1f}% ({original_player_correct}/{len(results)})\")\n",
    "print(f\"Minimal prompt:  {minimal_player_acc:.1f}% ({minimal_player_correct}/{len(results)})\")\n",
    "\n",
    "# Show player attribution errors\n",
    "print(\"\\n--- Minimal Prompt Player Errors ---\")\n",
    "player_errors = [r for r in results if not player_match(r[\"minimal\"][\"p\"], r[\"expected_player\"])]\n",
    "print(f\"Total: {len(player_errors)}/{len(results)}\")\n",
    "\n",
    "for r in player_errors:\n",
    "    text_preview = r['text'][:50] + \"...\" if len(r['text']) > 50 else r['text']\n",
    "    print(f\"\\n  Text: \\\"{text_preview}\\\"\")\n",
    "    print(f\"  Expected: {r['expected_player']}\")\n",
    "    print(f\"  Minimal:  {r['minimal']['p']}\")\n",
    "    print(f\"  Original: {r['original']['p']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798e578",
   "metadata": {},
   "source": [
    "## Structured Output Test\n",
    "\n",
    "### Compare manual JSON instruction vs Anthropic's native structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d4bba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class SentimentResult(BaseModel):\n",
    "    s: Literal[\"pos\", \"neg\", \"neu\"]\n",
    "    c: float  # confidence 0.0-1.0\n",
    "    p: str | None  # player name or null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d80b010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROMPT COMPARISON ===\n",
      "\n",
      "Manual JSON prompt:     287 chars\n",
      "Structured prompt:      207 chars\n",
      "Difference:             80 chars saved\n"
     ]
    }
   ],
   "source": [
    "# Minimal prompt WITHOUT JSON formatting instruction\n",
    "# (structured output handles the format)\n",
    "def build_structured_prompt(comment_body: str) -> str:\n",
    "    return f\"\"\"Classify sentiment toward NBA players. \n",
    "Slang: nasty/sick/filthy=positive, washed/brick/fraud/cooked=negative, GOAT=positive.\n",
    "\n",
    "Comment: {comment_body}\"\"\"\n",
    "\n",
    "# Compare prompt lengths\n",
    "test_comment = sample_comments[0][\"body\"]\n",
    "print(\"=== PROMPT COMPARISON ===\\n\")\n",
    "print(f\"Manual JSON prompt:     {len(build_minimal_prompt(test_comment))} chars\")\n",
    "print(f\"Structured prompt:      {len(build_structured_prompt(test_comment))} chars\")\n",
    "print(f\"Difference:             {len(build_minimal_prompt(test_comment)) - len(build_structured_prompt(test_comment))} chars saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51d15bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing structured output...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Structured: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:38<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structured output token stats (n=20):\n",
      "  Input  - Mean: 96.9\n",
      "  Output - Mean: 98.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Count input tokens WITH structured output\n",
    "# Note: count_tokens doesn't support output_format, so we measure via actual API call\n",
    "\n",
    "print(\"Testing structured output...\\n\")\n",
    "\n",
    "structured_results = []\n",
    "for comment in tqdm(sample_comments[:20], desc=\"Structured\"):\n",
    "    response = client.beta.messages.parse(\n",
    "        model=MODEL,\n",
    "        max_tokens=100,\n",
    "        temperature=0.0,\n",
    "        messages=[{\"role\": \"user\", \"content\": build_structured_prompt(comment[\"body\"])}],\n",
    "    )\n",
    "    structured_results.append({\n",
    "        \"input_tokens\": response.usage.input_tokens,\n",
    "        \"output_tokens\": response.usage.output_tokens,\n",
    "        \"response\": response.content[0].text\n",
    "    })\n",
    "\n",
    "print(\"\\nStructured output token stats (n=20):\")\n",
    "print(f\"  Input  - Mean: {statistics.mean(r['input_tokens'] for r in structured_results):.1f}\")\n",
    "print(f\"  Output - Mean: {statistics.mean(r['output_tokens'] for r in structured_results):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0561ffe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b7a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d23712d7",
   "metadata": {},
   "source": [
    "## Summary: Cost Optimization Journey\n",
    "\n",
    "### Initial Problem\n",
    "\n",
    "Our napkin-math estimates were wildly off:\n",
    "\n",
    "| Metric | Original Estimate | Actual (Measured) |\n",
    "|--------|-------------------|-------------------|\n",
    "| Input tokens/comment | ~150 | **261** |\n",
    "| Output tokens/comment | ~50 | **112** |\n",
    "| Total cost | ~$142-284 | **$1,156** |\n",
    "\n",
    "**Root causes:**\n",
    "1. System prompt overhead was 209 tokens alone (80% of input)\n",
    "2. Output tokens are 5× more expensive than input ($2.50 vs $0.50 per MTok)\n",
    "3. `reasoning` field generated ~60-80 tokens with no downstream use\n",
    "\n",
    "### Filter Quality Issues\n",
    "\n",
    "Player mention distribution revealed false positives:\n",
    "\n",
    "| Player | Sample % | Problem |\n",
    "|--------|----------|---------|\n",
    "| Aaron Gordon | 31.6% | `\"ag\"` with no word boundary gets substring matched |\n",
    "| Brandon Ingram | 16.9% | `\"bi\"` too ambiguous |\n",
    "\n",
    "These two players alone accounted for **48.5%** of \"player mentions\" — clearly wrong.\n",
    "\n",
    "### Optimization Strategy\n",
    "\n",
    "**Lever 1: Eliminate System Prompt**\n",
    "- Moved all context into user message\n",
    "- Haiku doesn't need hand-holding for simple classification\n",
    "\n",
    "**Lever 2: Compact Output Schema**\n",
    "- Before: `{\"sentiment\": \"negative\", \"confidence\": 0.85, \"target_player\": \"LeBron James\", \"reasoning\": \"...\"}`\n",
    "- After: `{\"s\": \"neg\", \"c\": 0.85, \"p\": \"LeBron James\"}`\n",
    "- Dropped `reasoning` entirely — no downstream use case\n",
    "\n",
    "**Lever 3: Clean Player Config**\n",
    "- Removed/fixed ambiguous aliases (`ag`, `bi`)\n",
    "- Added problematic short aliases to word-boundary matching list\n",
    "- Result: 31% reduction in matched comments (false positives eliminated)\n",
    "\n",
    "### Final Prompt\n",
    "```python\n",
    "def build_minimal_prompt(comment_body: str) -> str:\n",
    "    return f\"\"\"Classify sentiment toward NBA players. \n",
    "Slang: nasty/sick/filthy=positive, washed/brick/fraud/cooked=negative, GOAT=positive.\n",
    "\n",
    "Comment: {comment_body}\n",
    "\n",
    "Respond ONLY with JSON: {{\"s\":\"pos|neg|neu\",\"c\":0.0-1.0,\"p\":\"Player Name\"|null}}\"\"\"\n",
    "```\n",
    "\n",
    "### Results\n",
    "\n",
    "**Cost Reduction: 80%**\n",
    "\n",
    "|                         | Before      | After       | Change |\n",
    "|-------------------------|-------------|-------------|--------|\n",
    "| Input tokens/comment    | 261         | 128         | -51%   |\n",
    "| Output tokens/comment   | 112         | 23          | -79%   |\n",
    "| Player-mention comments | 2,814,947   | 1,939,268   | -31%   |\n",
    "| **Total cost**          | **$1,155.68** | **$235.73** | **-80%** |\n",
    "\n",
    "**Accuracy: No Degradation**\n",
    "\n",
    "| Metric | Original Prompt | Minimal Prompt |\n",
    "|--------|-----------------|----------------|\n",
    "| Sentiment accuracy | 95.7% | 95.7% |\n",
    "| Player attribution | 100% | 95.7%* |\n",
    "| Prompt agreement | — | 100% |\n",
    "\n",
    "*Only \"error\" was `AD` instead of `Anthony Davis` — correct identification, just abbreviated. Handle in post-processing.\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "1. **Measure tokens, don't estimate.** Our estimates were off by 4× because we didn't account for system prompt overhead.\n",
    "\n",
    "2. **Output tokens dominate cost.** At 5× the price of input, every output token matters. Kill unnecessary fields.\n",
    "\n",
    "3. **Smaller models don't need verbose prompts.** Haiku understood the task just as well with 128 tokens as 261. The extra context was wasted money.\n",
    "\n",
    "4. **Filter quality > filter coverage.** 31% of our \"player mentions\" were false positives. Garbage in, garbage out.\n",
    "\n",
    "5. **Validate accuracy before optimizing for cost.** We confirmed 95.7% accuracy and 100% prompt agreement before committing to the minimal prompt.\n",
    "\n",
    "### Go/No-Go\n",
    "\n",
    "✅ **CONDITIONAL GO**\n",
    "\n",
    "- Cost ($235.73) exceeds $200 target but within $300 ceiling\n",
    "- Accuracy validated (95.7% sentiment, 100% effective player attribution)\n",
    "- Filter quality improved (false positives eliminated)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run `scripts/filter_player_mentions.py` to generate filtered dataset\n",
    "2. Build batch processing pipeline (Phase 4)\n",
    "3. Run smoke test (10K comments) before full batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d03fc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
